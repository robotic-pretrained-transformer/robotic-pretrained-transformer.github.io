<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <link rel="stylesheet" href="main.css">
    <link rel="icon" type="image/x-icon" href="assets/favicon.png">
    <title>Robot Learning with Sensorimotor Pre-training</title>
</head>
<body>
<div id="title_slide">
    <div class="title_left">
        <h1>Robot Learning with Sensorimotor Pre-training</h1>
        <div class="author-container">
            <div class="grid-item"><a href="https://people.eecs.berkeley.edu/~ilija/">Ilija Radosavovic</a></div>
            <div class="grid-item"><a href="https://bfshi.github.io/">Baifeng Shi</a></div>
            <div class="grid-item"><a href="https://max-fu.github.io/">Letian Fu</a></div>
            <div class="grid-item"><a href="https://goldberg.berkeley.edu/">Ken Goldberg</a></div>
            <div class="grid-item"><a href="http://people.eecs.berkeley.edu/~trevor/">Trevor Darrell*</a></div>
            <div class="grid-item"><a href="http://people.eecs.berkeley.edu/~malik/">Jitendra Malik*</a></div>
        </div>
        <div class="mobile-author-container-1">
            <div class="grid-item"><a href="https://people.eecs.berkeley.edu/~ilija/">Ilija Radosavovic</a></div>
            <div class="grid-item"><a href="https://bfshi.github.io/">Baifeng Shi</a></div>
            <div class="grid-item"><a href="https://max-fu.github.io/">Letian Fu</a></div>
        </div>
        <div class="mobile-author-container-2">
            <div class="grid-item"><a href="https://goldberg.berkeley.edu/">Ken Goldberg</a></div>
            <div class="grid-item"><a href="http://people.eecs.berkeley.edu/~trevor/">Trevor Darrell*</a></div>
            <div class="grid-item"><a href="http://people.eecs.berkeley.edu/~malik/">Jitendra Malik*</a></div>
        </div>
        <div class="berkeley">
            <p>University of California, Berkeley</p>
        </div>
        <div class="button-container">
            <a href="http://arxiv.org/abs/2306.10007" class="button">Paper</a>
            <a href="" class="button">Code</a>
        </div>
        <div id="abstract" class="grid-container">
            <p>
                We present a self-supervised sensorimotor pre-training approach for robotics. Our model, called RPT, is a Transformer that operates on sequences of sensorimotor tokens. Given a sequence of camera images, proprioceptive robot states, and past actions, we encode the interleaved sequence into tokens, mask out a random subset, and train a model to predict the masked-out content. We hypothesize that if the robot can predict the missing content it has acquired a good model of the physical world that can enable it to act. RPT is designed to operate on latent visual representations which makes prediction tractable, enables scaling to 10x larger models, and 10 Hz inference on a real robot. To evaluate our approach, we collect a dataset of 20,000 real-world trajectories over 9 months using a combination of motion planning and model-based grasping algorithms. We find that pre-training on this data consistently outperforms training from scratch, leads to 2x improvements in the block stacking task, and has favorable scaling properties.
            </p>
        </div>
    </div>
</div>
<hr class="rounded">
<div id="overview">
    <p>
        We propose a self-supervised sensorimotor pre-training approach for robotics. We formulate pre-training via a sensorimotor prediction problem. We hypothesize that if the robot can predict the missing sensorimotor content it has acquired a good model about the physical world that can enable it to act. We instantiate this idea through a masked prediction task, similar to BERT/MAE.
    </p>

    <h1>Masked Sensorimotor Prediction</h1>

    <div class="approach">
        <div class="video_container">
            <video loop autoplay muted playsinline preload="metadata">
                <source src="assets/method_animation_v4.m4v" type="video/mp4">
            </video>
            <div class="caption">
                <p>Given a sequence of camera images, proprioceptive robot states, and past robot actions, we encode the interleaved sequence into tokens, mask out a random subset, and train a model to predict the masked-out content from the rest.</p>
            </div>
        </div>
    </div>

    <p>
        Our model, called RPT, is a Transformer that operates on sequences of sensorimotor tokens. Given an input sequence of camera images, proprioceptive states, and actions, we encode the interleaved sequence into tokens, mask out a random subset of the sequence, and predict the masked-out content from the rest. We perform random masking across all modalities and time using a high masking ratio, which encourages the model to learn cross-modal, spatio-temporal representations.
        <br><br>
        We encode camera images using a <a href="https://tetexiao.com/projects/real-mvp">pre-trained vision encoder</a> and use latent visual representations for sensorimotor sequence learning. This enables us to benefit from diverse visual data from the Internet that does not come with the accompanying proprioceptive and action information. Compared to prediction in pixel space, using latent visual representations makes the task more tractable. Finally, this design decouples the vision flops from the sensorimotor context length, making 10 Hz control with over 300M parameter models feasible on a physical robot.
    </p>

    <h1>Real-World Trajectories</h1>
    <p>
      To evaluate our pre-training approach, we collect a dataset of 20,000 real-world trajectories using a combination of motion planning and model-based grasping algorithms. Each trajectory is a sequence of multi-view camera images, proprioceptive robot states, and actions. We include classic motor control and robotic tasks, namely, single object picking, bin picking, stacking, and destacking, with variations in object poses, shape, and appearance.
    </p>

    <!--
    <div class="allegroupper">
        <video autoplay muted playsinline loop preload="metadata">
            <source src="assets/franka/pick-green-cube_05-22-2023/output_rgb_left.mp4" type="video/mp4">
        </video>
        <video autoplay muted playsinline loop preload="metadata">
            <source src="assets/franka/pick-green-cube_05-22-2023/output_rgb_hand.mp4" type="video/mp4">
        </video>
        <video autoplay muted playsinline loop preload="metadata">
            <source src="assets/franka/pick-green-cube_05-22-2023/output_rgb_right.mp4" type="video/mp4">
        </video>
    </div>
    <div class="allegrolower">
        <div class="video_container">
            <video autoplay muted playsinline loop preload="metadata">
                <source src="assets/franka/pick-wood-cylinder_05-29-2023/output_rgb_left.mp4" type="video/mp4">
            </video>
            <div class="caption">
                <p>Each trajectory is a sequence of camera images from three views, proprioceptive states, and actions</p>
            </div>
        </div>
        <div class="video_container">
            <video autoplay muted playsinline loop preload="metadata">
                <source src="assets/franka/pick-wood-cylinder_05-29-2023/output_rgb_hand.mp4" type="video/mp4">
            </video>
        </div>
        <div class="video_container">
            <video autoplay muted playsinline loop preload="metadata">
                <source src="assets/franka/pick-wood-cylinder_05-29-2023/output_rgb_right.mp4" type="video/mp4">
            </video>
        </div>
    </div>
    -->

    <div class="allegroupper">
        <video autoplay muted playsinline loop preload="metadata">
            <source src="assets/franka/pick-bin_05-26-2023/output_rgb_left.mp4" type="video/mp4">
        </video>
        <video autoplay muted playsinline loop preload="metadata">
            <source src="assets/franka/pick-bin_05-26-2023/output_rgb_hand.mp4" type="video/mp4">
        </video>
        <video autoplay muted playsinline loop preload="metadata">
            <source src="assets/franka/pick-bin_05-26-2023/output_rgb_right.mp4" type="video/mp4">
        </video>
    </div>
    <div class="allegrolower">
        <div class="video_container">
            <video autoplay muted playsinline loop preload="metadata">
                <source src="assets/franka/pick-bin_05-28-2023/output_rgb_left.mp4" type="video/mp4">
            </video>
            <div class="caption">
                <p>Each trajectory contains camera images from three views</p>
            </div>
        </div>
        <div class="video_container">
            <video autoplay muted playsinline loop preload="metadata">
                <source src="assets/franka/pick-bin_05-28-2023/output_rgb_hand.mp4" type="video/mp4">
            </video>
        </div>
        <div class="video_container">
            <video autoplay muted playsinline loop preload="metadata">
                <source src="assets/franka/pick-bin_05-28-2023/output_rgb_right.mp4" type="video/mp4">
            </video>
        </div>
    </div>


    <h1>Outperforms Training from Scratch</h1>
    <p>
        We study the effectiveness of masked pre-training by comparing fine-tuning performances with or without pre-training. We consider three evaluation tasks of increasing difficulty: picking, destacking, and stacking. To isolate the effect of pre-training alone, we use unseen data from the same task for pre-training and fine-tune on held-out data. Below, we show the performance as the number of fine-tuning demonstrations increases. We observe that pre-training leads to consistent improvements over training from scratch, across tasks and data regimes. Moreover, the gains are larger for the harder block stacking task.
    </p>
    <div class="barplot">
        <div class="image_container">
            <div class="caption">
                <p>Comparisons with training from scratch</p>
            </div>
            <img src="assets/fine_tune_curves.png" alt="Fine-tune curves.">
        </div>
    </div>

    <h1>Can Leverage Different Pre-training Data</h1>
    <p>
        Next, we study the impact of different pre-training data on fine-tuning performance. Specifically, we first pre-train our model on data from different tasks: stacking, picking, successful bin picking trajectories, and all bin picking trajectories. We then fine-tune and evaluate the model on stacking. Below, we see that RPT obtains similar performance on stacking when pre-trained on stacking, picking, or bin picking, suggesting that it is able to learn sensorimotor representations from data of different tasks. We also observe a lower performance when pre-trained on imperfect (both successful and failed) trajectories of bin picking, highlighting the importance of pre-training data quality.
    </p>
    <div class="barplot">
        <div class="image_container">
            <div class="caption">
                <p>Pre-training on different data</p>
            </div>
            <img style='width: 95%' src="assets/pretrain_data.png" alt="Pre-training data.">
        </div>
    </div>

    <h1>High Masking Ratio Across All Modalities</h1>
    <p>
        Finally, we perform ablation studies on different masking types and ratios. First, we find that masking across both modalities and times (token masking) is considerably more effective than masking one modality or one step at the time. Second, we find that masking with a high-masking ratio is important for good performance.
    </p>
    <div class="barplot">
        <div class="image_container">
            <div class="caption">
                <p>Masking types (left and middle) and masking ratio (right)</p>
            </div>
            <img src="assets/masking_ablation.png" alt="Masking ablations.">
        </div>
    </div>

    <h1>BibTeX</h1>
    <p class="bibtex">
        @article{Rpt2023,<br>
        &nbsp;&nbsp;title={Robot Learning with Sensorimotor Pre-training},<br>
        &nbsp;&nbsp;author={Ilija Radosavovic and Baifeng Shi and Letian Fu and Ken Goldberg and Trevor Darrell and Jitendra Malik},<br>
        &nbsp;&nbsp;year={2023},<br>
        &nbsp;&nbsp;journal={arXiv:2306.10007}<br>
        }
    </p>
</div>
<script type="text/javascript">
    /* https://stackoverflow.com/questions/3027707/how-to-change-the-playing-speed-of-videos-in-html5 */
    document.querySelector('video').defaultPlaybackRate = 1.0;
    document.querySelector('video').play();

    var videos =document.querySelectorAll('video');
    for (var i=0;i<1;i++)
    {
        videos[i].playbackRate = 1.0;
    }
</script>
<script>
    /* https://stackoverflow.com/questions/21163756/html5-and-javascript-to-play-videos-only-when-visible */
    var videos = document.getElementsByTagName("video");

    function checkScroll() {
        var fraction = 0.5; // Play when 70% of the player is visible.

        for(var i = 0; i < 1; i++) {  // only apply to the first video

            var video = videos[i];

            var x = video.offsetLeft, y = video.offsetTop, w = video.offsetWidth, h = video.offsetHeight, r = x + w, //right
                b = y + h, //bottom
                visibleX, visibleY, visible;

            visibleX = Math.max(0, Math.min(w, window.pageXOffset + window.innerWidth - x, r - window.pageXOffset));
            visibleY = Math.max(0, Math.min(h, window.pageYOffset + window.innerHeight - y, b - window.pageYOffset));

            visible = visibleX * visibleY / (w * h);

            if (visible > fraction) {
                video.play();
            } else {
                video.pause();
            }

        }

    }
    window.addEventListener('scroll', checkScroll, false);
    window.addEventListener('resize', checkScroll, false);
</script>
<script src="https://d3e54v103j8qbb.cloudfront.net/js/jquery-3.5.1.min.dc5e7f18c8.js?site=51e0d73d83d06baa7a00000f"
        type="text/javascript" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0="
        crossorigin="anonymous"></script>
<script src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/js/webflow.fd002feec.js"
        type="text/javascript"></script>
</body>
</html>
